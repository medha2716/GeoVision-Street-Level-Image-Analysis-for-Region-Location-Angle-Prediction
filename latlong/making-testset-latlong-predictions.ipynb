{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "383dbfbc",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-04T03:32:59.200686Z",
     "iopub.status.busy": "2025-05-04T03:32:59.200471Z",
     "iopub.status.idle": "2025-05-04T03:33:18.219628Z",
     "shell.execute_reply": "2025-05-04T03:33:18.218557Z"
    },
    "papermill": {
     "duration": 19.023085,
     "end_time": "2025-05-04T03:33:18.220893",
     "exception": false,
     "start_time": "2025-05-04T03:32:59.197808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model from /kaggle/input/lat-long-precictor/pytorch/default/1/best_geo_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/809496737.py:122: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(MODEL_PATH, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: lat_min=217805, lat_max=221696, lon_min=140523, lon_max=146067\n",
      "Number of regions: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B1_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B1_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b1_rwightman-bac287d4.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b1_rwightman-bac287d4.pth\n",
      "100%|██████████| 30.1M/30.1M [00:00<00:00, 169MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data from /kaggle/input/predicted-regions-test2/predicted_regions_test.csv\n",
      "Creating dataset with 369 images\n",
      "Making predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:06<00:00,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to predictions_test.csv\n",
      "\n",
      "Sample predictions:\n",
      "       filename       Latitude      Longitude\n",
      "0  img_0000.jpg  219614.659668  144893.706055\n",
      "1  img_0001.jpg  219190.734497  145357.229980\n",
      "2  img_0002.jpg  219874.955078  144570.798584\n",
      "3  img_0003.jpg  219995.219238  142058.842651\n",
      "4  img_0004.jpg  220219.393555  142283.443481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Constants\n",
    "IMAGE_SIZE = (256, 256)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Fixed paths - adjust these according to your setup\n",
    "MODEL_PATH = '/kaggle/input/lat-long-precictor/pytorch/default/1/best_geo_model.pth'\n",
    "TEST_REGIONS_PATH = '/kaggle/input/predicted-regions-test2/predicted_regions_test.csv'\n",
    "TEST_IMAGES_DIR = '/kaggle/input/smai-proj-test-set/images_test'\n",
    "OUTPUT_FILE = 'predictions_test.csv'\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define the model architecture (needs to match the saved model)\n",
    "class GeoModel(nn.Module):\n",
    "    def __init__(self, num_regions, backbone_name='efficientnet_b1'):\n",
    "        super(GeoModel, self).__init__()\n",
    "        \n",
    "        # Select backbone\n",
    "        if backbone_name == 'efficientnet_b1':\n",
    "            self.backbone = models.efficientnet_b1(pretrained=True)\n",
    "            feature_dim = 1280\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone: {backbone_name}\")\n",
    "            \n",
    "        self.backbone.classifier = nn.Identity()  # Remove classifier\n",
    "        \n",
    "        # Region embedding\n",
    "        self.region_embedding = nn.Embedding(num_regions, 128)\n",
    "        \n",
    "        # Region-based attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, feature_dim),\n",
    "            nn.Sigmoid()  # Output attention weights in [0,1]\n",
    "        )\n",
    "        \n",
    "        # Feature fusion with attention\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(feature_dim + 128, 768),\n",
    "            nn.BatchNorm1d(768),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(768, 384),\n",
    "            nn.BatchNorm1d(384),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(384, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        # Separate heads for latitude and longitude\n",
    "        self.lat_head = nn.Linear(64, 1)\n",
    "        self.lon_head = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, regions):\n",
    "        # Extract image features\n",
    "        img_features = self.backbone(x)\n",
    "        \n",
    "        # Get region embeddings\n",
    "        region_features = self.region_embedding(regions)\n",
    "        \n",
    "        # Generate attention weights based on region\n",
    "        attention_weights = self.attention(region_features)\n",
    "        \n",
    "        # Apply attention to image features\n",
    "        attended_features = img_features * attention_weights\n",
    "        \n",
    "        # Concatenate attended features with region embedding\n",
    "        combined_features = torch.cat([attended_features, region_features], dim=1)\n",
    "        \n",
    "        # Shared feature processing\n",
    "        shared_features = self.fusion(combined_features)\n",
    "        \n",
    "        # Separate predictions for latitude and longitude\n",
    "        lat = self.sigmoid(self.lat_head(shared_features)).squeeze(-1)\n",
    "        lon = self.sigmoid(self.lon_head(shared_features)).squeeze(-1)\n",
    "        \n",
    "        # Combine predictions\n",
    "        return torch.stack([lat, lon], dim=1)\n",
    "\n",
    "# Dataset class for test data\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, img_dir):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(IMAGE_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.df.iloc[idx]['filename'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        region = torch.tensor(self.df.iloc[idx]['region_encoded'], dtype=torch.long)\n",
    "        \n",
    "        return image, region, self.df.iloc[idx]['filename']\n",
    "\n",
    "def main():\n",
    "    print(f\"Loading model from {MODEL_PATH}\")\n",
    "    \n",
    "    # Load the model checkpoint\n",
    "    checkpoint = torch.load(MODEL_PATH, map_location=device)\n",
    "    \n",
    "    # Extract parameters from checkpoint\n",
    "    lat_min = checkpoint['lat_min']\n",
    "    lat_max = checkpoint['lat_max']\n",
    "    lon_min = checkpoint['lon_min']\n",
    "    lon_max = checkpoint['lon_max']\n",
    "    region_encoder = checkpoint['region_encoder']\n",
    "    num_regions = len(region_encoder.classes_)\n",
    "    \n",
    "    print(f\"Model parameters: lat_min={lat_min}, lat_max={lat_max}, lon_min={lon_min}, lon_max={lon_max}\")\n",
    "    print(f\"Number of regions: {num_regions}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = GeoModel(num_regions, backbone_name='efficientnet_b1').to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Loading test data from {TEST_REGIONS_PATH}\")\n",
    "    \n",
    "    # Load test data\n",
    "    test_df = pd.read_csv(TEST_REGIONS_PATH)\n",
    "    \n",
    "    # Encode regions\n",
    "    test_df['region_encoded'] = region_encoder.transform(test_df['Region_ID'])\n",
    "    \n",
    "    print(f\"Creating dataset with {len(test_df)} images\")\n",
    "    \n",
    "    # Create test dataset and dataloader\n",
    "    test_dataset = TestDataset(test_df, TEST_IMAGES_DIR)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Make predictions\n",
    "    all_preds = []\n",
    "    all_filenames = []\n",
    "    \n",
    "    print(\"Making predictions...\")\n",
    "    with torch.no_grad():\n",
    "        for images, regions, filenames in tqdm(test_loader):\n",
    "            images = images.to(device)\n",
    "            regions = regions.to(device)\n",
    "            outputs = model(images, regions)\n",
    "            \n",
    "            # Denormalize predictions\n",
    "            pred_lats = outputs[:, 0].cpu().numpy() * (lat_max - lat_min) + lat_min\n",
    "            pred_lons = outputs[:, 1].cpu().numpy() * (lon_max - lon_min) + lon_min\n",
    "            \n",
    "            for i in range(len(filenames)):\n",
    "                all_preds.append((pred_lats[i], pred_lons[i]))\n",
    "                all_filenames.append(filenames[i])\n",
    "    \n",
    "    # Create dataframe with predictions\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'filename': all_filenames,\n",
    "        'Latitude': [pred[0] for pred in all_preds],\n",
    "        'Longitude': [pred[1] for pred in all_preds]\n",
    "    })\n",
    "    \n",
    "    # Save predictions\n",
    "    predictions_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"Predictions saved to {OUTPUT_FILE}\")\n",
    "    \n",
    "    # Display sample predictions\n",
    "    print(\"\\nSample predictions:\")\n",
    "    print(predictions_df.head(5))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11893923,
     "sourceId": 99530,
     "sourceType": "competition"
    },
    {
     "datasetId": 7317331,
     "sourceId": 11660094,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7321664,
     "sourceId": 11666131,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7322351,
     "sourceId": 11667284,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 328306,
     "modelInstanceId": 307855,
     "sourceId": 371961,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 25.211012,
   "end_time": "2025-05-04T03:33:20.345095",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-04T03:32:55.134083",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
