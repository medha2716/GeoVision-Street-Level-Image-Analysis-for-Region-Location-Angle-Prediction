{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4fbcc32",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-03T18:21:47.304371Z",
     "iopub.status.busy": "2025-05-03T18:21:47.304147Z",
     "iopub.status.idle": "2025-05-03T18:22:05.698759Z",
     "shell.execute_reply": "2025-05-03T18:22:05.697725Z"
    },
    "papermill": {
     "duration": 18.398744,
     "end_time": "2025-05-03T18:22:05.700083",
     "exception": false,
     "start_time": "2025-05-03T18:21:47.301339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Number of unique regions in test data: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
      "100%|██████████| 20.5M/20.5M [00:00<00:00, 119MB/s] \n",
      "/tmp/ipykernel_19/71489937.py:170: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model from /kaggle/input/best_model/pytorch/default/1/best_model_angle.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|██████████| 12/12 [00:03<00:00,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved predictions to 2022101034_1.csv\n",
      "Total predictions: 369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the dataset class for test data\n",
    "class TestAngleDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transform=None):\n",
    "        self.df = df.copy()\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.img_dir, row['filename'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # Return image, region, and filename\n",
    "        region = torch.tensor(row['region_encoded'], dtype=torch.long)\n",
    "        return image, region, row['filename']\n",
    "\n",
    "# Define the model architecture (same as in training)\n",
    "class ImprovedAngleModel(nn.Module):\n",
    "    def __init__(self, num_regions, backbone_name='efficientnet_b0'):\n",
    "        super(ImprovedAngleModel, self).__init__()\n",
    "        \n",
    "        # Choose backbone based on parameter\n",
    "        if backbone_name == 'resnet50':\n",
    "            self.backbone = models.resnet50(pretrained=True)\n",
    "            self.backbone.fc = nn.Identity()  # Remove final FC layer\n",
    "            self.feature_dim = 2048\n",
    "        elif backbone_name == 'efficientnet_b0':\n",
    "            self.backbone = models.efficientnet_b0(pretrained=True)\n",
    "            self.backbone.classifier = nn.Identity()  # Remove classifier\n",
    "            self.feature_dim = 1280\n",
    "        elif backbone_name == 'convnext_small':\n",
    "            self.backbone = models.convnext_small(pretrained=True)\n",
    "            self.backbone.classifier[2] = nn.Identity()  # Remove classifier\n",
    "            self.feature_dim = 768\n",
    "        else:  # Default to ResNet18\n",
    "            self.backbone = models.resnet18(pretrained=True)\n",
    "            self.backbone.fc = nn.Identity()  # Remove final FC layer\n",
    "            self.feature_dim = 512\n",
    "        \n",
    "        # Region embedding\n",
    "        self.region_embedding = nn.Embedding(num_regions, 128)\n",
    "        \n",
    "        # Attention mechanism for feature fusion\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.feature_dim + 128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        # Final layers with multiple branches\n",
    "        self.shared_fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_dim + 128, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.angle_fc = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 2)  # Output sin(θ), cos(θ)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, regions):\n",
    "        # Extract image features\n",
    "        img_features = self.backbone(x)\n",
    "        \n",
    "        # Get region embeddings\n",
    "        region_features = self.region_embedding(regions)\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat([img_features, region_features], dim=1)\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        attention_weights = torch.sigmoid(self.attention(combined_features))\n",
    "        attended_features = combined_features * attention_weights\n",
    "        \n",
    "        # Shared layers\n",
    "        shared_features = self.shared_fc(attended_features)\n",
    "        \n",
    "        # Angle prediction branch\n",
    "        angle_out = self.angle_fc(shared_features)\n",
    "        \n",
    "        return angle_out\n",
    "\n",
    "# Function to get test transforms\n",
    "def get_test_transform(img_size=288, crop_size=256):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def main():\n",
    "    # Set seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device: {device}')\n",
    "\n",
    "    # Path configurations\n",
    "    MODEL_PATH = '/kaggle/input/best_model/pytorch/default/1/best_model_angle.pth'  # Path to the saved model\n",
    "    TEST_IMG_DIR = '/kaggle/input/images-test/images_test'  # Path to test images\n",
    "    REGIONS_PATH = '/kaggle/input/predicted-regions-test/predicted_regions_test.csv'  # Path to predicted regions\n",
    "    \n",
    "    # Get test transform\n",
    "    test_transform = get_test_transform(img_size=288, crop_size=256)\n",
    "    \n",
    "    # Load test data\n",
    "    test_df = pd.read_csv(REGIONS_PATH)\n",
    "    \n",
    "    # Load region encodings from training data to ensure consistency\n",
    "    # The simplest approach is to use the same regions as in training\n",
    "    # For this example, we'll re-encode regions in this script\n",
    "    # In practice, you should save the LabelEncoder during training\n",
    "    region_encoder = LabelEncoder()\n",
    "    \n",
    "    # We'll assume regions from the test data might match what we had during training\n",
    "    # If you have the original encoder or region mapping, use that instead\n",
    "    test_df['region_encoded'] = region_encoder.fit_transform(test_df['Region_ID'])\n",
    "    \n",
    "    # Get number of unique regions\n",
    "    NUM_REGIONS = len(region_encoder.classes_)\n",
    "    print(f\"Number of unique regions in test data: {NUM_REGIONS}\")\n",
    "    \n",
    "    # Create test dataset\n",
    "    test_dataset = TestAngleDataset(test_df, TEST_IMG_DIR, test_transform)\n",
    "    \n",
    "    # Create test dataloader\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=32, \n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Initialize model with the same architecture\n",
    "    backbone_name = 'efficientnet_b0'  # Make sure this matches what was used in training\n",
    "    model = ImprovedAngleModel(NUM_REGIONS, backbone_name=backbone_name).to(device)\n",
    "    \n",
    "    # Load weights\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "        print(f\"Successfully loaded model from {MODEL_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Generate predictions\n",
    "    filenames = []\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, regions, batch_filenames in tqdm(test_loader, desc='Generating predictions'):\n",
    "            images = images.to(device)\n",
    "            regions = regions.to(device)\n",
    "            \n",
    "            outputs = model(images, regions)\n",
    "            outputs = F.normalize(outputs, dim=1)\n",
    "            \n",
    "            # Convert sin(θ), cos(θ) predictions back to angles\n",
    "            sin_preds = outputs[:, 0]\n",
    "            cos_preds = outputs[:, 1]\n",
    "            angle_preds = (torch.atan2(sin_preds, cos_preds) * 180 / np.pi) % 360\n",
    "            \n",
    "            filenames.extend(batch_filenames)\n",
    "            predictions.extend(angle_preds.cpu().numpy())\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results_df = pd.DataFrame({\n",
    "        'filename': filenames,\n",
    "        'angle': predictions\n",
    "    })\n",
    "    \n",
    "    # Sort by filename to ensure consistent ordering\n",
    "    results_df = results_df.sort_values(by='filename')\n",
    "    \n",
    "    # Create submission file with id and angle columns\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': range(len(results_df)),\n",
    "        'angle': results_df['angle'].values\n",
    "    })\n",
    "    \n",
    "    # Save submission file\n",
    "    submission_df.to_csv('2022101034_1.csv', index=False)\n",
    "    print(f\"Successfully saved predictions to 2022101034_1.csv\")\n",
    "    print(f\"Total predictions: {len(submission_df)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11893923,
     "isSourceIdPinned": false,
     "sourceId": 99530,
     "sourceType": "competition"
    },
    {
     "datasetId": 7320113,
     "sourceId": 11663951,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7320395,
     "sourceId": 11664321,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7320419,
     "sourceId": 11664354,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 327528,
     "modelInstanceId": 307050,
     "sourceId": 370925,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 328023,
     "modelInstanceId": 307560,
     "sourceId": 371553,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 26.573905,
   "end_time": "2025-05-03T18:22:08.919748",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-03T18:21:42.345843",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
